{
  "run_id": "execute_phase-mlu3py6e-e1e2c9",
  "tool_name": "gsd_execute_phase",
  "workflow_file": "execute-phase.md",
  "workflow_args": {
    "phase": "12"
  },
  "created_at": "2026-02-19T23:37:53.990Z",
  "updated_at": "2026-02-20T00:07:43.742Z",
  "status": "completed",
  "next_step": 2,
  "total_steps": 2,
  "preamble": "<arguments>\n{\n  \"phase\": \"12\"\n}\n</arguments>\n\n<mcp_safe_execution>\nSubagent delegation is not available in many MCP clients.\nExecute every single-agent step directly in this same session.\nUse intermediate files as handoff boundaries between passes to preserve context quality.\nIf original text mentions \"spawn\" or \"subagent\", interpret it as \"run the next pass yourself\".\n</mcp_safe_execution>\n\n<next_step_policy>\nAfter completing EACH major step, your response MUST end with a line:\nNext on list: <exact next command/tool and args>\nDo not end a step response without this line.\nAfter workflow completion: Next on list -> if this phase has unresolved gaps, call gsd_plan_phase with {\"phase\":\"X\",\"gaps\":\"true\"}; otherwise call gsd_discuss_phase for the next roadmap phase.\n</next_step_policy>\n\n<purpose>\nExecute all plans in a phase using wave-based parallel execution. Orchestrator stays lean — delegates plan execution to single-agent passes.\n</purpose>\n\n<core_principle>\nOrchestrator coordinates, not executes. Each single-agent pass loads the full execute-plan context. Orchestrator: discover plans → analyze deps → group waves → run agents → handle checkpoints → collect results.\n</core_principle>\n\n<required_reading>\nRead STATE.md before any operation to load project context.\n</required_reading>\n\n<process>\n\n<step name=\"initialize\" priority=\"first\">\nCall the `gsd_init_execute_phase` tool with `{ \"phase\": \"{PHASE_ARG}\" }`. Parse the returned JSON for: `executor_model`, `verifier_model`, `commit_docs`, `parallelization`, `branching_strategy`, `branch_name`, `phase_found`, `phase_dir`, `phase_number`, `phase_name`, `phase_slug`, `plans`, `incomplete_plans`, `plan_count`, `incomplete_count`, `state_exists`, `roadmap_exists`.\n\n**If `phase_found` is false:** Error — phase directory not found.\n**If `plan_count` is 0:** Error — no plans found in phase.\n**If `state_exists` is false but `.planning/` exists:** Offer reconstruct or continue.\n\nWhen `parallelization` is false, plans within a wave execute sequentially.\n</step>\n\n<step name=\"handle_branching\">\nCheck `branching_strategy` from init:\n\n**\"none\":** Skip, continue on current branch.\n\n**\"phase\" or \"milestone\":** Use pre-computed `branch_name` from init:\n```bash\ngit checkout -b \"$BRANCH_NAME\" 2>/dev/null || git checkout \"$BRANCH_NAME\"\n```\n\nAll subsequent commits go to this branch. User handles merging.\n</step>\n\n<step name=\"validate_phase\">\nFrom init JSON: `phase_dir`, `plan_count`, `incomplete_count`.\n\nReport: \"Found {plan_count} plans in {phase_dir} ({incomplete_count} incomplete)\"\n</step>\n\n<step name=\"discover_and_group_plans\">\nCall the `gsd_get_phase_plan` tool with `{ \"phase\": \"{PHASE_NUMBER}\" }`. Parse the returned JSON for: `phase`, `plans[]` (each with `id`, `wave`, `autonomous`, `objective`, `files_modified`, `task_count`, `has_summary`), `waves` (map of wave number → plan IDs), `incomplete`, `has_checkpoints`.\n\n**Filtering:** Skip plans where `has_summary: true`. If `--gaps-only`: also skip non-gap_closure plans. If all filtered: \"No matching incomplete plans\" → exit.\n\nReport:\n```\n## Execution Plan\n\n**Phase {X}: {Name}** — {total_plans} plans across {wave_count} waves\n\n| Wave | Plans | What it builds |\n|------|-------|----------------|\n| 1 | 01-01, 01-02 | {from plan objectives, 3-8 words} |\n| 2 | 01-03 | ... |\n```\n</step>\n\n<step name=\"execute_waves\">\nExecute each wave in sequence. Within a wave: parallel if `PARALLELIZATION=true`, sequential if `false`.\n\n**For each wave:**\n\n1. **Describe what's being built (BEFORE running):**\n\n   Read each plan's `<objective>`. Extract what's being built and why.\n\n   ```\n   ---\n   ## Wave {N}\n\n   **{Plan ID}: {Plan Name}**\n   {2-3 sentences: what this builds, technical approach, why it matters}\n\n   Running {count} agent(s)...\n   ---\n   ```\n\n   - Bad: \"Executing terrain generation plan\"\n   - Good: \"Procedural terrain generator using Perlin noise — creates height maps, biome zones, and collision meshes. Required before vehicle physics can interact with ground.\"\n\n2. **Run executor agents:**\n\n   Pass paths only — executors read files themselves with their fresh 200k context.\n   This keeps orchestrator context lean (~10-15%).\n\n   ```",
  "epilogue": "```\n\nRead status:\n```bash\ngrep \"^status:\" \"$PHASE_DIR\"/*-VERIFICATION.md | cut -d: -f2 | tr -d ' '\n```\n\n| Status | Action |\n|--------|--------|\n| `passed` | → update_roadmap |\n| `human_needed` | Present items for human testing, get approval or feedback |\n| `gaps_found` | Present gap summary, offer `gsd_plan_phase` tool with `{ \"phase\": \"{X}\", \"gaps\": \"true\" }` |\n\n**If human_needed:**\n```\n## ✓ Phase {X}: {Name} — Human Verification Required\n\nAll automated checks passed. {N} items need human testing:\n\n{From VERIFICATION.md human_verification section}\n\n\"approved\" → continue | Report issues → gap closure\n```\n\n**If gaps_found:**\n```\n## ⚠ Phase {X}: {Name} — Gaps Found\n\n**Score:** {N}/{M} must-haves verified\n**Report:** {phase_dir}/{phase_num}-VERIFICATION.md\n\n### What's Missing\n{Gap summaries from VERIFICATION.md}\n\n---\n## ▶ Next Up\n\nCall the `gsd_plan_phase` tool with `{ \"phase\": \"{X}\", \"gaps\": \"true\" }`\n\n<sub>Start a fresh conversation for best results</sub>\n\nAlso: `cat {phase_dir}/{phase_num}-VERIFICATION.md` — full report\nAlso: Call the `gsd_verify_work` tool with `{ \"phase\": \"{X}\" }` — manual testing first\n```\n\nGap closure cycle: Call the `gsd_plan_phase` tool with `{ \"phase\": \"{X}\", \"gaps\": \"true\" }` — reads VERIFICATION.md → creates gap plans with `gap_closure: true` → user calls the `gsd_execute_phase` tool with `{ \"phase\": \"{X}\", \"gaps\": \"true\" }` → verifier re-runs.\n</step>\n\n<step name=\"update_roadmap\">\n**Mark phase complete and update all tracking files:**\n\nCall the `gsd_complete_phase` tool with `{ \"phase\": \"{PHASE_NUMBER}\" }`.\n\nThe tool handles:\n- Marking phase checkbox `[x]` with completion date\n- Updating Progress table (Status → Complete, date)\n- Updating plan count to final\n- Advancing STATE.md to next phase\n- Updating REQUIREMENTS.md traceability\n\nExtract from result: `next_phase`, `next_phase_name`, `is_last_phase`.\n\nCall the `gsd_commit_work` tool with `{ \"message\": \"docs(phase-{X}): complete phase execution\", \"files\": \".planning/ROADMAP.md .planning/STATE.md .planning/REQUIREMENTS.md .planning/phases/{phase_dir}/*-VERIFICATION.md\" }`.\n</step>\n\n<step name=\"offer_next\">\n\n**Exception:** If `gaps_found`, the `verify_phase_goal` step already presents the gap-closure path (call the `gsd_plan_phase` tool with `{ \"phase\": \"{X}\", \"gaps\": \"true\" }`). No additional routing needed — skip auto-advance.\n\n**Auto-advance detection:**\n\n1. Check for `\"auto\": \"true\"` in the `<arguments>` JSON block above\n2. Call the `gsd_config_get` tool with `{ \"key\": \"workflow.auto_advance\" }`. Use the returned value as `AUTO_CFG`.\n\n**If `\"auto\": \"true\"` is present in arguments OR `AUTO_CFG` is true (AND verification passed with no gaps):**\n\n```\n╔══════════════════════════════════════════╗\n║  AUTO-ADVANCING → TRANSITION             ║\n║  Phase {X} verified, continuing chain    ║\n╚══════════════════════════════════════════╝\n```\n\nExecute the transition workflow inline (do NOT use delegate — orchestrator context is ~10-15%, transition needs phase completion data already in context):\n\nRead and follow `~/.claude/get-shit-done/workflows/transition.md`, passing through the `\"auto\": \"true\"` argument so it propagates to the next phase invocation.\n\n**If neither `\"auto\"` argument nor `AUTO_CFG` is true:**\n\nThe workflow ends. The user calls the `gsd_progress` tool or invokes the transition workflow manually.\n</step>\n\n</process>\n\n<context_efficiency>\nOrchestrator: ~10-15% context. Single-agent passes: fresh 200k each. No polling (Task blocks). No context bleed.\n</context_efficiency>\n\n<failure_handling>\n- **Agent fails mid-plan:** Missing SUMMARY.md → report, ask user how to proceed\n- **Dependency chain breaks:** Wave 1 fails → Wave 2 dependents likely fail → user chooses attempt or skip\n- **All agents in wave fail:** Systemic issue → stop, report for investigation\n- **Checkpoint unresolvable:** \"Skip this plan?\" or \"Abort phase execution?\" → record partial progress in STATE.md\n</failure_handling>\n\n<resumption>\nRe-run the `gsd_execute_phase` tool with `{ \"phase\": \"{phase}\" }` → discover_plans finds completed SUMMARYs → skips them → resumes from first incomplete plan → continues wave execution.\n\nSTATE.md tracks: last completed plan, current wave, pending checkpoints.\n</resumption>\n</output>",
  "steps": [
    {
      "number": "1",
      "content": "<single_agent_step number=\"1\">\nExecute this pass directly in the current conversation (MCP-safe mode: no delegated call).\nOriginal delegated task: Execute plan {plan_number} of phase {phase_number}-{phase_name}\n\n<iterative_execution>\nThis step represents a fanout/loop from the original single-agent pass workflow.\nExecute exactly ONE item in this agent run (one plan, one gap, one dimension, etc.).\nIf more items remain, continue this same step using step_result=\"retry\" in a fresh agent.\nOnly send step_result=\"done\" when all items for this step are complete.\n</iterative_execution>\n\n\n<task_prompt>\n<objective>\n       Execute plan {plan_number} of phase {phase_number}-{phase_name}.\n       Commit each task atomically. Create SUMMARY.md. Update STATE.md and ROADMAP.md.\n       </objective>\n\n       <execution_context>\n       <purpose>\nExecute a phase prompt (PLAN.md) and create the outcome summary (SUMMARY.md).\n</purpose>\n\n<required_reading>\nRead STATE.md before any operation to load project context.\nRead config.json for planning behavior settings.\n\n@~/.claude/get-shit-done/references/git-integration.md\n</required_reading>\n\n<process>\n\n<step name=\"init_context\" priority=\"first\">\nLoad execution context:\n\nCall the `gsd_init_execute_phase` tool with `{ \"phase\": \"{PHASE}\", \"include\": \"state,config\" }`. Parse the returned JSON for: `executor_model`, `commit_docs`, `phase_dir`, `phase_number`, `plans`, `summaries`, `incomplete_plans`.\n\n**File contents (from --include):** `state_content`, `config_content`.\n\nIf `.planning/` missing: error.\n</step>\n\n<step name=\"identify_plan\">\n```bash\n# Use plans/summaries from init JSON, or list files\nls .planning/phases/XX-name/*-PLAN.md 2>/dev/null | sort\nls .planning/phases/XX-name/*-SUMMARY.md 2>/dev/null | sort\n```\n\nFind first PLAN without matching SUMMARY. Decimal phases supported (`01.1-hotfix/`):\n\n```bash\nPHASE=$(echo \"$PLAN_PATH\" | grep -oE '[0-9]+(\\.[0-9]+)?-[0-9]+')\n# config_content already loaded via --include config in init_context\n```\n\n<if mode=\"yolo\">\nAuto-approve: `⚡ Execute {phase}-{plan}-PLAN.md [Plan X of Y for Phase Z]` → parse_segments.\n</if>\n\n<if mode=\"interactive\" OR=\"custom with gates.execute_next_plan true\">\nPresent plan identification, wait for confirmation.\n</if>\n</step>\n\n<step name=\"record_start_time\">\n```bash\nPLAN_START_TIME=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\nPLAN_START_EPOCH=$(date +%s)\n```\n</step>\n\n<step name=\"parse_segments\">\n```bash\ngrep -n \"type=\\\"checkpoint\" .planning/phases/XX-name/{phase}-{plan}-PLAN.md\n```\n\n**Routing by checkpoint type:**\n\n| Checkpoints | Pattern | Execution |\n|-------------|---------|-----------|\n| None | A (autonomous) | Single single-agent pass: full plan + SUMMARY + commit |\n| Verify-only | B (segmented) | Segments between checkpoints. After none/human-verify → Single-agent pass. After decision/human-action → MAIN |\n| Decision | C (main) | Execute entirely in main context |\n\n**Pattern A:** init_agent_tracking → run delegate with type=\"gsd-executor\", model=executor_model, prompt: execute plan at [path], autonomous, all tasks + SUMMARY + commit, follow deviation/auth rules, report: plan name, tasks, SUMMARY path, commit hash → track agent_id → wait → update tracking → report.\n\n**Pattern B:** Execute segment-by-segment. Autonomous segments: run single-agent pass for assigned tasks only (no SUMMARY/commit). Checkpoints: main context. After all segments: aggregate, create SUMMARY, commit. See segment_execution.\n\n**Pattern C:** Execute in main using standard flow (step name=\"execute\").\n\nFresh context per single-agent pass preserves peak quality. Main context stays lean.\n</step>\n\n<step name=\"init_agent_tracking\">\n```bash\nif [ ! -f .planning/agent-history.json ]; then\n  echo '{\"version\":\"1.0\",\"max_entries\":50,\"entries\":[]}' > .planning/agent-history.json\nfi\nrm -f .planning/current-agent-id.txt\nif [ -f .planning/current-agent-id.txt ]; then\n  INTERRUPTED_ID=$(cat .planning/current-agent-id.txt)\n  echo \"Found interrupted agent: $INTERRUPTED_ID\"\nfi\n```\n\nIf interrupted: ask user to resume or start fresh.\n\n**Tracking protocol:** On run: write agent_id to `current-agent-id.txt`, append to agent-history.json: `{\"agent_id\":\"[id]\",\"task_description\":\"[desc]\",\"phase\":\"[phase]\",\"plan\":\"[plan]\",\"segment\":[num|null],\"timestamp\":\"[ISO]\",\"status\":\"run\",\"completion_timestamp\":null}`. On completion: status → \"completed\", set completion_timestamp, delete current-agent-id.txt. Prune: if entries > max_entries, remove oldest \"completed\" (never \"run\").\n\nRun for Pattern A/B before running. Pattern C: skip.\n</step>\n\n<step name=\"segment_execution\">\nPattern B only (verify-only checkpoints). Skip for A/C.\n\n1. Parse segment map: checkpoint locations and types\n2. Per segment:\n   - Single-agent pass route: run gsd-executor for assigned tasks only. Prompt: task range, plan path, read full plan for context, execute assigned tasks, track deviations, NO SUMMARY/commit. Track via agent protocol.\n   - Main route: execute tasks using standard flow (step name=\"execute\")\n3. After ALL segments: aggregate files/deviations/decisions → create SUMMARY.md → commit → self-check:\n   - Verify key-files.created exist on disk with `[ -f ]`\n   - Check `git log --oneline --all --grep=\"{phase}-{plan}\"` returns ≥1 commit\n   - Append `## Self-Check: PASSED` or `## Self-Check: FAILED` to SUMMARY\n\n</step>\n\n<step name=\"load_prompt\">\n```bash\ncat .planning/phases/XX-name/{phase}-{plan}-PLAN.md\n```\nThis IS the execution instructions. Follow exactly. If plan references CONTEXT.md: honor user's vision throughout.\n</step>\n\n<step name=\"previous_phase_check\">\nCall the `gsd_list_phases` tool with `{ \"type\": \"summaries\" }`. Extract the second-to-last summary from the JSON result.\n\nIf previous SUMMARY has unresolved \"Issues Encountered\" or \"Next Phase Readiness\" blockers:\n\n<prompt_user>\n  <question header=\"Previous Issues\">Previous phase has unresolved issues. How to proceed?</question>\n  <option label=\"Proceed anyway\">Continue despite unresolved issues</option>\n  <option label=\"Address first\">Resolve issues before continuing</option>\n  <option label=\"Review previous\">Review the previous phase summary</option>\n</prompt_user>\n</step>\n\n<step name=\"execute\">\nDeviations are normal — handle via rules below.\n\n1. Read @context files from prompt\n2. Per task:\n   - `type=\"auto\"`: if `tdd=\"true\"` → TDD execution. Implement with deviation rules + auth gates. Verify done criteria. Commit (see task_commit). Track hash for Summary.\n   - `type=\"checkpoint:*\"`: STOP → checkpoint_protocol → wait for user → continue only after confirmation.\n3. Run `<verification>` checks\n4. Confirm `<success_criteria>` met\n5. Document deviations in Summary\n</step>\n\n<authentication_gates>\n\n## Authentication Gates\n\nAuth errors during execution are NOT failures — they're expected interaction points.\n\n**Indicators:** \"Not authenticated\", \"Unauthorized\", 401/403, \"Please run {tool} login\", \"Set {ENV_VAR}\"\n\n**Protocol:**\n1. Recognize auth gate (not a bug)\n2. STOP task execution\n3. Create dynamic checkpoint:human-action with exact auth steps\n4. Wait for user to authenticate\n5. Verify credentials work\n6. Retry original task\n7. Continue normally\n\n**Example:** `vercel --yes` → \"Not authenticated\" → checkpoint asking user to `vercel login` → verify with `vercel whoami` → retry deploy → continue\n\n**In Summary:** Document as normal flow under \"## Authentication Gates\", not as deviations.\n\n</authentication_gates>\n\n<deviation_rules>\n\n## Deviation Rules\n\nYou WILL discover unplanned work. Apply automatically, track all for Summary.\n\n| Rule | Trigger | Action | Permission |\n|------|---------|--------|------------|\n| **1: Bug** | Broken behavior, errors, wrong queries, type errors, security vulns, race conditions, leaks | Fix → test → verify → track `[Rule 1 - Bug]` | Auto |\n| **2: Missing Critical** | Missing essentials: error handling, validation, auth, CSRF/CORS, rate limiting, indexes, logging | Add → test → verify → track `[Rule 2 - Missing Critical]` | Auto |\n| **3: Blocking** | Prevents completion: missing deps, wrong types, broken imports, missing env/config/files, circular deps | Fix blocker → verify proceeds → track `[Rule 3 - Blocking]` | Auto |\n| **4: Architectural** | Structural change: new DB table, schema change, new service, switching libs, breaking API, new infra | STOP → present decision (below) → track `[Rule 4 - Architectural]` | Ask user |\n\n**Rule 4 format:**\n```\n⚠️ Architectural Decision Needed\n\nCurrent task: [task name]\nDiscovery: [what prompted this]\nProposed change: [modification]\nWhy needed: [rationale]\nImpact: [what this affects]\nAlternatives: [other approaches]\n\nProceed with proposed change? (yes / different approach / defer)\n```\n\n**Priority:** Rule 4 (STOP) > Rules 1-3 (auto) > unsure → Rule 4\n**Edge cases:** missing validation → R2 | null crash → R1 | new table → R4 | new column → R1/2\n**Heuristic:** Affects correctness/security/completion? → R1-3. Maybe? → R4.\n\n</deviation_rules>\n\n<deviation_documentation>\n\n## Documenting Deviations\n\nSummary MUST include deviations section. None? → `## Deviations from Plan\\n\\nNone - plan executed exactly as written.`\n\nPer deviation: **[Rule N - Category] Title** — Found during: Task X | Issue | Fix | Files modified | Verification | Commit hash\n\nEnd with: **Total deviations:** N auto-fixed (breakdown). **Impact:** assessment.\n\n</deviation_documentation>\n\n<tdd_plan_execution>\n## TDD Execution\n\nFor `type: tdd` plans — RED-GREEN-REFACTOR:\n\n1. **Infrastructure** (first TDD plan only): detect project, install framework, config, verify empty suite\n2. **RED:** Read `<behavior>` → failing test(s) → run (MUST fail) → commit: `test({phase}-{plan}): add failing test for [feature]`\n3. **GREEN:** Read `<implementation>` → minimal code → run (MUST pass) → commit: `feat({phase}-{plan}): implement [feature]`\n4. **REFACTOR:** Clean up → tests MUST pass → commit: `refactor({phase}-{plan}): clean up [feature]`\n\nErrors: RED doesn't fail → investigate test/existing feature. GREEN doesn't pass → debug, iterate. REFACTOR breaks → undo.\n\nSee `~/.claude/get-shit-done/references/tdd.md` for structure.\n</tdd_plan_execution>\n\n<task_commit>\n## Task Commit Protocol\n\nAfter each task (verification passed, done criteria met), commit immediately.\n\n**1. Check:** `git status --short`\n\n**2. Stage individually** (NEVER `git add .` or `git add -A`):\n```bash\ngit add src/api/auth.ts\ngit add src/types/user.ts\n```\n\n**3. Commit type:**\n\n| Type | When | Example |\n|------|------|---------|\n| `feat` | New functionality | feat(08-02): create user registration endpoint |\n| `fix` | Bug fix | fix(08-02): correct email validation regex |\n| `test` | Test-only (TDD RED) | test(08-02): add failing test for password hashing |\n| `refactor` | No behavior change (TDD REFACTOR) | refactor(08-02): extract validation to helper |\n| `perf` | Performance | perf(08-02): add database index |\n| `docs` | Documentation | docs(08-02): add API docs |\n| `style` | Formatting | style(08-02): format auth module |\n| `chore` | Config/deps | chore(08-02): add bcrypt dependency |\n\n**4. Format:** `{type}({phase}-{plan}): {description}` with bullet points for key changes.\n\n**5. Record hash:**\n```bash\nTASK_COMMIT=$(git rev-parse --short HEAD)\nTASK_COMMITS+=(\"Task ${TASK_NUM}: ${TASK_COMMIT}\")\n```\n\n</task_commit>\n\n<step name=\"checkpoint_protocol\">\nOn `type=\"checkpoint:*\"`: automate everything possible first. Checkpoints are for verification/decisions only.\n\nDisplay: `CHECKPOINT: [Type]` box → Progress {X}/{Y} → Task name → type-specific content → `YOUR ACTION: [signal]`\n\n| Type | Content | Resume signal |\n|------|---------|---------------|\n| human-verify (90%) | What was built + verification steps (commands/URLs) | \"approved\" or describe issues |\n| decision (9%) | Decision needed + context + options with pros/cons | \"Select: option-id\" |\n| human-action (1%) | What was automated + ONE manual step + verification plan | \"done\" |\n\nAfter response: verify if specified. Pass → continue. Fail → inform, wait. WAIT for user — do NOT hallucinate completion.\n\nSee ~/.claude/get-shit-done/references/checkpoints.md for details.\n</step>\n\n<step name=\"checkpoint_return_for_orchestrator\">\nWhen run via delegate and hitting checkpoint: return structured state (cannot interact with user directly).\n\n**Required return:** 1) Completed Tasks table (hashes + files) 2) Current Task (what's blocking) 3) Checkpoint Details (user-facing content) 4) Awaiting (what's needed from user)\n\nOrchestrator parses → presents to user → runs fresh continuation with your completed tasks state. You will NOT be resumed. In main context: use checkpoint_protocol above.\n</step>\n\n<step name=\"verification_failure_gate\">\nIf verification fails: STOP. Present: \"Verification failed for Task [X]: [name]. Expected: [criteria]. Actual: [result].\" Options: Retry | Skip (mark incomplete) | Stop (investigate). If skipped → SUMMARY \"Issues Encountered\".\n</step>\n\n<step name=\"record_completion_time\">\n```bash\nPLAN_END_TIME=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\nPLAN_END_EPOCH=$(date +%s)\n\nDURATION_SEC=$(( PLAN_END_EPOCH - PLAN_START_EPOCH ))\nDURATION_MIN=$(( DURATION_SEC / 60 ))\n\nif [[ $DURATION_MIN -ge 60 ]]; then\n  HRS=$(( DURATION_MIN / 60 ))\n  MIN=$(( DURATION_MIN % 60 ))\n  DURATION=\"${HRS}h ${MIN}m\"\nelse\n  DURATION=\"${DURATION_MIN} min\"\nfi\n```\n</step>\n\n<step name=\"generate_user_setup\">\n```bash\ngrep -A 50 \"^user_setup:\" .planning/phases/XX-name/{phase}-{plan}-PLAN.md | head -50\n```\n\nIf user_setup exists: create `{phase}-USER-SETUP.md` using template `~/.claude/get-shit-done/templates/user-setup.md`. Per service: env vars table, account setup checklist, dashboard config, local dev notes, verification commands. Status \"Incomplete\". Set `USER_SETUP_CREATED=true`. If empty/missing: skip.\n</step>\n\n<step name=\"create_summary\">\nCreate `{phase}-{plan}-SUMMARY.md` at `.planning/phases/XX-name/`. Use `~/.claude/get-shit-done/templates/summary.md`.\n\n**Frontmatter:** phase, plan, subsystem, tags | requires/provides/affects | tech-stack.added/patterns | key-files.created/modified | key-decisions | requirements-completed (**MUST** copy `requirements` array from PLAN.md frontmatter verbatim) | duration ($DURATION), completed ($PLAN_END_TIME date).\n\nTitle: `# Phase [X] Plan [Y]: [Name] Summary`\n\nOne-liner SUBSTANTIVE: \"JWT auth with refresh rotation using jose library\" not \"Authentication implemented\"\n\nInclude: duration, start/end times, task count, file count.\n\nNext: more plans → \"Ready for {next-plan}\" | last → \"Phase complete, ready for transition\".\n</step>\n\n<step name=\"update_current_position\">\nUpdate STATE.md using tools:\n\nCall the `gsd_state_advance_plan` tool. This advances the plan counter (handles last-plan edge case).\n\nCall the `gsd_state_update_progress` tool. This recalculates the progress bar from disk state.\n\nCall the `gsd_state_record_metric` tool with `{ \"phase\": \"{PHASE}\", \"plan\": \"{PLAN}\", \"duration\": \"{DURATION}\", \"tasks\": \"{TASK_COUNT}\", \"files\": \"{FILE_COUNT}\" }`.\n</step>\n\n<step name=\"extract_decisions_and_issues\">\nFrom SUMMARY: Extract decisions and add to STATE.md:\n\nCall the `gsd_state_add_decision` tool with `{ \"phase\": \"{PHASE}\", \"summary\": \"{DECISION_TEXT}\", \"rationale\": \"{RATIONALE}\" }` for each decision from SUMMARY key-decisions.\n\nFor blockers, call the `gsd_state_patch` tool to add blocker description.\n</step>\n\n<step name=\"update_session_continuity\">\nCall the `gsd_state_record_session` tool with `{ \"stopped_at\": \"Completed ${PHASE}-${PLAN}-PLAN.md\", \"resume_file\": \"None\" }`.\n\nKeep STATE.md under 150 lines.\n</step>\n\n<step name=\"issues_review_gate\">\nIf SUMMARY \"Issues Encountered\" ≠ \"None\": yolo → log and continue. Interactive → present issues, wait for acknowledgment.\n</step>\n\n<step name=\"update_roadmap\">\nCall the `gsd_roadmap_update_plan_progress` tool with `{ \"phase\": \"{PHASE}\" }`.\n\nCounts PLAN vs SUMMARY files on disk. Updates progress table row with correct count and status (`In Progress` or `Complete` with date).\n</step>\n\n<step name=\"update_requirements\">\nMark completed requirements from the PLAN.md frontmatter `requirements:` field:\n\nCall the `gsd_requirements_mark_complete` tool with `{ \"ids\": \"{REQ_IDS}\" }`.\n\nExtract requirement IDs from the plan's frontmatter (e.g., `requirements: [AUTH-01, AUTH-02]`). If no requirements field, skip.\n</step>\n\n<step name=\"git_commit_metadata\">\nTask code already committed per-task. Commit plan metadata:\n\nCall the `gsd_commit_work` tool with `{ \"message\": \"docs({phase}-{plan}): complete [plan-name] plan\", \"files\": \".planning/phases/XX-name/{phase}-{plan}-SUMMARY.md .planning/STATE.md .planning/ROADMAP.md .planning/REQUIREMENTS.md\" }`.\n</step>\n\n<step name=\"update_codebase_map\">\nIf .planning/codebase/ doesn't exist: skip.\n\n```bash\nFIRST_TASK=$(git log --oneline --grep=\"feat({phase}-{plan}):\" --grep=\"fix({phase}-{plan}):\" --grep=\"test({phase}-{plan}):\" --reverse | head -1 | cut -d' ' -f1)\ngit diff --name-only ${FIRST_TASK}^..HEAD 2>/dev/null\n```\n\nUpdate only structural changes: new src/ dir → STRUCTURE.md | deps → STACK.md | file pattern → CONVENTIONS.md | API client → INTEGRATIONS.md | config → STACK.md | renamed → update paths. Skip code-only/bugfix/content changes.\n\nCall the `gsd_commit_work` tool with `{ \"message\": \"\", \"files\": \".planning/codebase/*.md\", \"amend\": true }`.\n</step>\n\n<step name=\"offer_next\">\nIf `USER_SETUP_CREATED=true`: display `⚠️ USER SETUP REQUIRED` with path + env/config tasks at TOP.\n\n```bash\nls -1 .planning/phases/[current-phase-dir]/*-PLAN.md 2>/dev/null | wc -l\nls -1 .planning/phases/[current-phase-dir]/*-SUMMARY.md 2>/dev/null | wc -l\n```\n\n| Condition | Route | Action |\n|-----------|-------|--------|\n| summaries < plans | **A: More plans** | Find next PLAN without SUMMARY. Yolo: auto-continue. Interactive: show next plan, suggest calling the `gsd_execute_phase` tool + the `gsd_verify_work` tool. STOP here. |\n| summaries = plans, current < highest phase | **B: Phase done** | Show completion, suggest calling the `gsd_plan_phase` tool with `{ \"phase\": \"{Z+1}\" }` + the `gsd_verify_work` tool with `{ \"phase\": \"{Z}\" }` + the `gsd_discuss_phase` tool with `{ \"phase\": \"{Z+1}\" }` |\n| summaries = plans, current = highest phase | **C: Milestone done** | Show banner, suggest calling the `gsd_complete_milestone` tool + the `gsd_verify_work` tool + the `gsd_add_phase` tool |\n\nAll routes: Start a fresh conversation for best results.\n</step>\n\n</process>\n\n<success_criteria>\n\n- All tasks from PLAN.md completed\n- All verifications pass\n- USER-SETUP.md generated if user_setup in frontmatter\n- SUMMARY.md created with substantive content\n- STATE.md updated (position, decisions, issues, session)\n- ROADMAP.md updated\n- If codebase map exists: map updated with execution changes (or skipped if no significant changes)\n- If USER-SETUP.md created: prominently surfaced in completion output\n</success_criteria>\n</output>\n\n       # Summary Template\n\nTemplate for `.planning/phases/XX-name/{phase}-{plan}-SUMMARY.md` - phase completion documentation.\n\n---\n\n## File Template\n\n```markdown\n---\nphase: XX-name\nplan: YY\nsubsystem: [primary category: auth, payments, ui, api, database, infra, testing, etc.]\ntags: [searchable tech: jwt, stripe, react, postgres, prisma]\n\n# Dependency graph\nrequires:\n  - phase: [prior phase this depends on]\n    provides: [what that phase built that this uses]\nprovides:\n  - [bullet list of what this phase built/delivered]\naffects: [list of phase names or keywords that will need this context]\n\n# Tech tracking\ntech-stack:\n  added: [libraries/tools added in this phase]\n  patterns: [architectural/code patterns established]\n\nkey-files:\n  created: [important files created]\n  modified: [important files modified]\n\nkey-decisions:\n  - \"Decision 1\"\n  - \"Decision 2\"\n\npatterns-established:\n  - \"Pattern 1: description\"\n  - \"Pattern 2: description\"\n\nrequirements-completed: []  # REQUIRED — Copy ALL requirement IDs from this plan's `requirements` frontmatter field.\n\n# Metrics\nduration: Xmin\ncompleted: YYYY-MM-DD\n---\n\n# Phase [X]: [Name] Summary\n\n**[Substantive one-liner describing outcome - NOT \"phase complete\" or \"implementation finished\"]**\n\n## Performance\n\n- **Duration:** [time] (e.g., 23 min, 1h 15m)\n- **Started:** [ISO timestamp]\n- **Completed:** [ISO timestamp]\n- **Tasks:** [count completed]\n- **Files modified:** [count]\n\n## Accomplishments\n- [Most important outcome]\n- [Second key accomplishment]\n- [Third if applicable]\n\n## Task Commits\n\nEach task was committed atomically:\n\n1. **Task 1: [task name]** - `abc123f` (feat/fix/test/refactor)\n2. **Task 2: [task name]** - `def456g` (feat/fix/test/refactor)\n3. **Task 3: [task name]** - `hij789k` (feat/fix/test/refactor)\n\n**Plan metadata:** `lmn012o` (docs: complete plan)\n\n_Note: TDD tasks may have multiple commits (test → feat → refactor)_\n\n## Files Created/Modified\n- `path/to/file.ts` - What it does\n- `path/to/another.ts` - What it does\n\n## Decisions Made\n[Key decisions with brief rationale, or \"None - followed plan as specified\"]\n\n## Deviations from Plan\n\n[If no deviations: \"None - plan executed exactly as written\"]\n\n[If deviations occurred:]\n\n### Auto-fixed Issues\n\n**1. [Rule X - Category] Brief description**\n- **Found during:** Task [N] ([task name])\n- **Issue:** [What was wrong]\n- **Fix:** [What was done]\n- **Files modified:** [file paths]\n- **Verification:** [How it was verified]\n- **Committed in:** [hash] (part of task commit)\n\n[... repeat for each auto-fix ...]\n\n---\n\n**Total deviations:** [N] auto-fixed ([breakdown by rule])\n**Impact on plan:** [Brief assessment - e.g., \"All auto-fixes necessary for correctness/security. No scope creep.\"]\n\n## Issues Encountered\n[Problems and how they were resolved, or \"None\"]\n\n[Note: \"Deviations from Plan\" documents unplanned work that was handled automatically via deviation rules. \"Issues Encountered\" documents problems during planned work that required problem-solving.]\n\n## User Setup Required\n\n[If USER-SETUP.md was generated:]\n**External services require manual configuration.** See [{phase}-USER-SETUP.md](./{phase}-USER-SETUP.md) for:\n- Environment variables to add\n- Dashboard configuration steps\n- Verification commands\n\n[If no USER-SETUP.md:]\nNone - no external service configuration required.\n\n## Next Phase Readiness\n[What's ready for next phase]\n[Any blockers or concerns]\n\n---\n*Phase: XX-name*\n*Completed: [date]*\n```\n\n<frontmatter_guidance>\n**Purpose:** Enable automatic context assembly via dependency graph. Frontmatter makes summary metadata machine-readable so plan-phase can scan all summaries quickly and select relevant ones based on dependencies.\n\n**Fast scanning:** Frontmatter is first ~25 lines, cheap to scan across all summaries without reading full content.\n\n**Dependency graph:** `requires`/`provides`/`affects` create explicit links between phases, enabling transitive closure for context selection.\n\n**Subsystem:** Primary categorization (auth, payments, ui, api, database, infra, testing) for detecting related phases.\n\n**Tags:** Searchable technical keywords (libraries, frameworks, tools) for tech stack awareness.\n\n**Key-files:** Important files for @context references in PLAN.md.\n\n**Patterns:** Established conventions future phases should maintain.\n\n**Population:** Frontmatter is populated during summary creation in execute-plan.md. See `<step name=\"create_summary\">` for field-by-field guidance.\n</frontmatter_guidance>\n\n<one_liner_rules>\nThe one-liner MUST be substantive:\n\n**Good:**\n- \"JWT auth with refresh rotation using jose library\"\n- \"Prisma schema with User, Session, and Product models\"\n- \"Dashboard with real-time metrics via Server-Sent Events\"\n\n**Bad:**\n- \"Phase complete\"\n- \"Authentication implemented\"\n- \"Foundation finished\"\n- \"All tasks done\"\n\nThe one-liner should tell someone what actually shipped.\n</one_liner_rules>\n\n<example>\n```markdown\n# Phase 1: Foundation Summary\n\n**JWT auth with refresh rotation using jose library, Prisma User model, and protected API middleware**\n\n## Performance\n\n- **Duration:** 28 min\n- **Started:** 2025-01-15T14:22:10Z\n- **Completed:** 2025-01-15T14:50:33Z\n- **Tasks:** 5\n- **Files modified:** 8\n\n## Accomplishments\n- User model with email/password auth\n- Login/logout endpoints with httpOnly JWT cookies\n- Protected route middleware checking token validity\n- Refresh token rotation on each request\n\n## Files Created/Modified\n- `prisma/schema.prisma` - User and Session models\n- `src/app/api/auth/login/route.ts` - Login endpoint\n- `src/app/api/auth/logout/route.ts` - Logout endpoint\n- `src/middleware.ts` - Protected route checks\n- `src/lib/auth.ts` - JWT helpers using jose\n\n## Decisions Made\n- Used jose instead of jsonwebtoken (ESM-native, Edge-compatible)\n- 15-min access tokens with 7-day refresh tokens\n- Storing refresh tokens in database for revocation capability\n\n## Deviations from Plan\n\n### Auto-fixed Issues\n\n**1. [Rule 2 - Missing Critical] Added password hashing with bcrypt**\n- **Found during:** Task 2 (Login endpoint implementation)\n- **Issue:** Plan didn't specify password hashing - storing plaintext would be critical security flaw\n- **Fix:** Added bcrypt hashing on registration, comparison on login with salt rounds 10\n- **Files modified:** src/app/api/auth/login/route.ts, src/lib/auth.ts\n- **Verification:** Password hash test passes, plaintext never stored\n- **Committed in:** abc123f (Task 2 commit)\n\n**2. [Rule 3 - Blocking] Installed missing jose dependency**\n- **Found during:** Task 4 (JWT token generation)\n- **Issue:** jose package not in package.json, import failing\n- **Fix:** Ran `npm install jose`\n- **Files modified:** package.json, package-lock.json\n- **Verification:** Import succeeds, build passes\n- **Committed in:** def456g (Task 4 commit)\n\n---\n\n**Total deviations:** 2 auto-fixed (1 missing critical, 1 blocking)\n**Impact on plan:** Both auto-fixes essential for security and functionality. No scope creep.\n\n## Issues Encountered\n- jsonwebtoken CommonJS import failed in Edge runtime - switched to jose (planned library change, worked as expected)\n\n## Next Phase Readiness\n- Auth foundation complete, ready for feature development\n- User registration endpoint needed before public launch\n\n---\n*Phase: 01-foundation*\n*Completed: 2025-01-15*\n```\n</example>\n\n<guidelines>\n**Frontmatter:** MANDATORY - complete all fields. Enables automatic context assembly for future planning.\n\n**One-liner:** Must be substantive. \"JWT auth with refresh rotation using jose library\" not \"Authentication implemented\".\n\n**Decisions section:**\n- Key decisions made during execution with rationale\n- Extracted to STATE.md accumulated context\n- Use \"None - followed plan as specified\" if no deviations\n\n**After creation:** STATE.md updated with position, decisions, issues.\n</guidelines>\n\n       <overview>\nPlans execute autonomously. Checkpoints formalize interaction points where human verification or decisions are needed.\n\n**Core principle:** Claude automates everything with CLI/API. Checkpoints are for verification and decisions, not manual work.\n\n**Golden rules:**\n1. **If Claude can run it, Claude runs it** - Never ask user to execute CLI commands, start servers, or run builds\n2. **Claude sets up the verification environment** - Start dev servers, seed databases, configure env vars\n3. **User only does what requires human judgment** - Visual checks, UX evaluation, \"does this feel right?\"\n4. **Secrets come from user, automation comes from Claude** - Ask for API keys, then Claude uses them via CLI\n5. **Auto-mode bypasses verification/decision checkpoints** — When `workflow.auto_advance` is true in config: human-verify auto-approves, decision auto-selects first option, human-action still stops (auth gates cannot be automated)\n</overview>\n\n<checkpoint_types>\n\n<type name=\"human-verify\">\n## checkpoint:human-verify (Most Common - 90%)\n\n**When:** Claude completed automated work, human confirms it works correctly.\n\n**Use for:**\n- Visual UI checks (layout, styling, responsiveness)\n- Interactive flows (click through wizard, test user flows)\n- Functional verification (feature works as expected)\n- Audio/video playback quality\n- Animation smoothness\n- Accessibility testing\n\n**Structure:**\n```xml\n<task type=\"checkpoint:human-verify\" gate=\"blocking\">\n  <what-built>[What Claude automated and deployed/built]</what-built>\n  <how-to-verify>\n    [Exact steps to test - URLs, commands, expected behavior]\n  </how-to-verify>\n  <resume-signal>[How to continue - \"approved\", \"yes\", or describe issues]</resume-signal>\n</task>\n```\n\n**Example: UI Component (shows key pattern: Claude starts server BEFORE checkpoint)**\n```xml\n<task type=\"auto\">\n  <name>Build responsive dashboard layout</name>\n  <files>src/components/Dashboard.tsx, src/app/dashboard/page.tsx</files>\n  <action>Create dashboard with sidebar, header, and content area. Use Tailwind responsive classes for mobile.</action>\n  <verify>npm run build succeeds, no TypeScript errors</verify>\n  <done>Dashboard component builds without errors</done>\n</task>\n\n<task type=\"auto\">\n  <name>Start dev server for verification</name>\n  <action>Run `npm run dev` in background, wait for \"ready\" message, capture port</action>\n  <verify>curl http://localhost:3000 returns 200</verify>\n  <done>Dev server running at http://localhost:3000</done>\n</task>\n\n<task type=\"checkpoint:human-verify\" gate=\"blocking\">\n  <what-built>Responsive dashboard layout - dev server running at http://localhost:3000</what-built>\n  <how-to-verify>\n    Visit http://localhost:3000/dashboard and verify:\n    1. Desktop (>1024px): Sidebar left, content right, header top\n    2. Tablet (768px): Sidebar collapses to hamburger menu\n    3. Mobile (375px): Single column layout, bottom nav appears\n    4. No layout shift or horizontal scroll at any size\n  </how-to-verify>\n  <resume-signal>Type \"approved\" or describe layout issues</resume-signal>\n</task>\n```\n\n**Example: Xcode Build**\n```xml\n<task type=\"auto\">\n  <name>Build macOS app with Xcode</name>\n  <files>App.xcodeproj, Sources/</files>\n  <action>Run `xcodebuild -project App.xcodeproj -scheme App build`. Check for compilation errors in output.</action>\n  <verify>Build output contains \"BUILD SUCCEEDED\", no errors</verify>\n  <done>App builds successfully</done>\n</task>\n\n<task type=\"checkpoint:human-verify\" gate=\"blocking\">\n  <what-built>Built macOS app at DerivedData/Build/Products/Debug/App.app</what-built>\n  <how-to-verify>\n    Open App.app and test:\n    - App launches without crashes\n    - Menu bar icon appears\n    - Preferences window opens correctly\n    - No visual glitches or layout issues\n  </how-to-verify>\n  <resume-signal>Type \"approved\" or describe issues</resume-signal>\n</task>\n```\n</type>\n\n<type name=\"decision\">\n## checkpoint:decision (9%)\n\n**When:** Human must make choice that affects implementation direction.\n\n**Use for:**\n- Technology selection (which auth provider, which database)\n- Architecture decisions (monorepo vs separate repos)\n- Design choices (color scheme, layout approach)\n- Feature prioritization (which variant to build)\n- Data model decisions (schema structure)\n\n**Structure:**\n```xml\n<task type=\"checkpoint:decision\" gate=\"blocking\">\n  <decision>[What's being decided]</decision>\n  <context>[Why this decision matters]</context>\n  <options>\n    <option id=\"option-a\">\n      <name>[Option name]</name>\n      <pros>[Benefits]</pros>\n      <cons>[Tradeoffs]</cons>\n    </option>\n    <option id=\"option-b\">\n      <name>[Option name]</name>\n      <pros>[Benefits]</pros>\n      <cons>[Tradeoffs]</cons>\n    </option>\n  </options>\n  <resume-signal>[How to indicate choice]</resume-signal>\n</task>\n```\n\n**Example: Auth Provider Selection**\n```xml\n<task type=\"checkpoint:decision\" gate=\"blocking\">\n  <decision>Select authentication provider</decision>\n  <context>\n    Need user authentication for the app. Three solid options with different tradeoffs.\n  </context>\n  <options>\n    <option id=\"supabase\">\n      <name>Supabase Auth</name>\n      <pros>Built-in with Supabase DB we're using, generous free tier, row-level security integration</pros>\n      <cons>Less customizable UI, tied to Supabase ecosystem</cons>\n    </option>\n    <option id=\"clerk\">\n      <name>Clerk</name>\n      <pros>Beautiful pre-built UI, best developer experience, excellent docs</pros>\n      <cons>Paid after 10k MAU, vendor lock-in</cons>\n    </option>\n    <option id=\"nextauth\">\n      <name>NextAuth.js</name>\n      <pros>Free, self-hosted, maximum control, widely adopted</pros>\n      <cons>More setup work, you manage security updates, UI is DIY</cons>\n    </option>\n  </options>\n  <resume-signal>Select: supabase, clerk, or nextauth</resume-signal>\n</task>\n```\n\n**Example: Database Selection**\n```xml\n<task type=\"checkpoint:decision\" gate=\"blocking\">\n  <decision>Select database for user data</decision>\n  <context>\n    App needs persistent storage for users, sessions, and user-generated content.\n    Expected scale: 10k users, 1M records first year.\n  </context>\n  <options>\n    <option id=\"supabase\">\n      <name>Supabase (Postgres)</name>\n      <pros>Full SQL, generous free tier, built-in auth, real-time subscriptions</pros>\n      <cons>Vendor lock-in for real-time features, less flexible than raw Postgres</cons>\n    </option>\n    <option id=\"planetscale\">\n      <name>PlanetScale (MySQL)</name>\n      <pros>Serverless scaling, branching workflow, excellent DX</pros>\n      <cons>MySQL not Postgres, no foreign keys in free tier</cons>\n    </option>\n    <option id=\"convex\">\n      <name>Convex</name>\n      <pros>Real-time by default, TypeScript-native, automatic caching</pros>\n      <cons>Newer platform, different mental model, less SQL flexibility</cons>\n    </option>\n  </options>\n  <resume-signal>Select: supabase, planetscale, or convex</resume-signal>\n</task>\n```\n</type>\n\n<type name=\"human-action\">\n## checkpoint:human-action (1% - Rare)\n\n**When:** Action has NO CLI/API and requires human-only interaction, OR Claude hit an authentication gate during automation.\n\n**Use ONLY for:**\n- **Authentication gates** - Claude tried CLI/API but needs credentials (this is NOT a failure)\n- Email verification links (clicking email)\n- SMS 2FA codes (phone verification)\n- Manual account approvals (platform requires human review)\n- Credit card 3D Secure flows (web-based payment authorization)\n- OAuth app approvals (web-based approval)\n\n**Do NOT use for pre-planned manual work:**\n- Deploying (use CLI - auth gate if needed)\n- Creating webhooks/databases (use API/CLI - auth gate if needed)\n- Running builds/tests (use Bash tool)\n- Creating files (use Write tool)\n\n**Structure:**\n```xml\n<task type=\"checkpoint:human-action\" gate=\"blocking\">\n  <action>[What human must do - Claude already did everything automatable]</action>\n  <instructions>\n    [What Claude already automated]\n    [The ONE thing requiring human action]\n  </instructions>\n  <verification>[What Claude can check afterward]</verification>\n  <resume-signal>[How to continue]</resume-signal>\n</task>\n```\n\n**Example: Email Verification**\n```xml\n<task type=\"auto\">\n  <name>Create SendGrid account via API</name>\n  <action>Use SendGrid API to create subuser account with provided email. Request verification email.</action>\n  <verify>API returns 201, account created</verify>\n  <done>Account created, verification email sent</done>\n</task>\n\n<task type=\"checkpoint:human-action\" gate=\"blocking\">\n  <action>Complete email verification for SendGrid account</action>\n  <instructions>\n    I created the account and requested verification email.\n    Check your inbox for SendGrid verification link and click it.\n  </instructions>\n  <verification>SendGrid API key works: curl test succeeds</verification>\n  <resume-signal>Type \"done\" when email verified</resume-signal>\n</task>\n```\n\n**Example: Authentication Gate (Dynamic Checkpoint)**\n```xml\n<task type=\"auto\">\n  <name>Deploy to Vercel</name>\n  <files>.vercel/, vercel.json</files>\n  <action>Run `vercel --yes` to deploy</action>\n  <verify>vercel ls shows deployment, curl returns 200</verify>\n</task>\n\n<!-- If vercel returns \"Error: Not authenticated\", Claude creates checkpoint on the fly -->\n\n<task type=\"checkpoint:human-action\" gate=\"blocking\">\n  <action>Authenticate Vercel CLI so I can continue deployment</action>\n  <instructions>\n    I tried to deploy but got authentication error.\n    Run: vercel login\n    This will open your browser - complete the authentication flow.\n  </instructions>\n  <verification>vercel whoami returns your account email</verification>\n  <resume-signal>Type \"done\" when authenticated</resume-signal>\n</task>\n\n<!-- After authentication, Claude retries the deployment -->\n\n<task type=\"auto\">\n  <name>Retry Vercel deployment</name>\n  <action>Run `vercel --yes` (now authenticated)</action>\n  <verify>vercel ls shows deployment, curl returns 200</verify>\n</task>\n```\n\n**Key distinction:** Auth gates are created dynamically when Claude encounters auth errors. NOT pre-planned — Claude automates first, asks for credentials only when blocked.\n</type>\n</checkpoint_types>\n\n<execution_protocol>\n\nWhen Claude encounters `type=\"checkpoint:*\"`:\n\n1. **Stop immediately** - do not proceed to next task\n2. **Display checkpoint clearly** using the format below\n3. **Wait for user response** - do not hallucinate completion\n4. **Verify if possible** - check files, run tests, whatever is specified\n5. **Resume execution** - continue to next task only after confirmation\n\n**For checkpoint:human-verify:**\n```\n╔═══════════════════════════════════════════════════════╗\n║  CHECKPOINT: Verification Required                    ║\n╚═══════════════════════════════════════════════════════╝\n\nProgress: 5/8 tasks complete\nTask: Responsive dashboard layout\n\nBuilt: Responsive dashboard at /dashboard\n\nHow to verify:\n  1. Visit: http://localhost:3000/dashboard\n  2. Desktop (>1024px): Sidebar visible, content fills remaining space\n  3. Tablet (768px): Sidebar collapses to icons\n  4. Mobile (375px): Sidebar hidden, hamburger menu appears\n\n────────────────────────────────────────────────────────\n→ YOUR ACTION: Type \"approved\" or describe issues\n────────────────────────────────────────────────────────\n```\n\n**For checkpoint:decision:**\n```\n╔═══════════════════════════════════════════════════════╗\n║  CHECKPOINT: Decision Required                        ║\n╚═══════════════════════════════════════════════════════╝\n\nProgress: 2/6 tasks complete\nTask: Select authentication provider\n\nDecision: Which auth provider should we use?\n\nContext: Need user authentication. Three options with different tradeoffs.\n\nOptions:\n  1. supabase - Built-in with our DB, free tier\n     Pros: Row-level security integration, generous free tier\n     Cons: Less customizable UI, ecosystem lock-in\n\n  2. clerk - Best DX, paid after 10k users\n     Pros: Beautiful pre-built UI, excellent documentation\n     Cons: Vendor lock-in, pricing at scale\n\n  3. nextauth - Self-hosted, maximum control\n     Pros: Free, no vendor lock-in, widely adopted\n     Cons: More setup work, DIY security updates\n\n────────────────────────────────────────────────────────\n→ YOUR ACTION: Select supabase, clerk, or nextauth\n────────────────────────────────────────────────────────\n```\n\n**For checkpoint:human-action:**\n```\n╔═══════════════════════════════════════════════════════╗\n║  CHECKPOINT: Action Required                          ║\n╚═══════════════════════════════════════════════════════╝\n\nProgress: 3/8 tasks complete\nTask: Deploy to Vercel\n\nAttempted: vercel --yes\nError: Not authenticated. Please run 'vercel login'\n\nWhat you need to do:\n  1. Run: vercel login\n  2. Complete browser authentication when it opens\n  3. Return here when done\n\nI'll verify: vercel whoami returns your account\n\n────────────────────────────────────────────────────────\n→ YOUR ACTION: Type \"done\" when authenticated\n────────────────────────────────────────────────────────\n```\n</execution_protocol>\n\n<authentication_gates>\n\n**Auth gate = Claude tried CLI/API, got auth error.** Not a failure — a gate requiring human input to unblock.\n\n**Pattern:** Claude tries automation → auth error → creates checkpoint:human-action → user authenticates → Claude retries → continues\n\n**Gate protocol:**\n1. Recognize it's not a failure - missing auth is expected\n2. Stop current task - don't retry repeatedly\n3. Create checkpoint:human-action dynamically\n4. Provide exact authentication steps\n5. Verify authentication works\n6. Retry the original task\n7. Continue normally\n\n**Key distinction:**\n- Pre-planned checkpoint: \"I need you to do X\" (wrong - Claude should automate)\n- Auth gate: \"I tried to automate X but need credentials\" (correct - unblocks automation)\n\n</authentication_gates>\n\n<automation_reference>\n\n**The rule:** If it has CLI/API, Claude does it. Never ask human to perform automatable work.\n\n## Service CLI Reference\n\n| Service | CLI/API | Key Commands | Auth Gate |\n|---------|---------|--------------|-----------|\n| Vercel | `vercel` | `--yes`, `env add`, `--prod`, `ls` | `vercel login` |\n| Railway | `railway` | `init`, `up`, `variables set` | `railway login` |\n| Fly | `fly` | `launch`, `deploy`, `secrets set` | `fly auth login` |\n| Stripe | `stripe` + API | `listen`, `trigger`, API calls | API key in .env |\n| Supabase | `supabase` | `init`, `link`, `db push`, `gen types` | `supabase login` |\n| Upstash | `upstash` | `redis create`, `redis get` | `upstash auth login` |\n| PlanetScale | `pscale` | `database create`, `branch create` | `pscale auth login` |\n| GitHub | `gh` | `repo create`, `pr create`, `secret set` | `gh auth login` |\n| Node | `npm`/`pnpm` | `install`, `run build`, `test`, `run dev` | N/A |\n| Xcode | `xcodebuild` | `-project`, `-scheme`, `build`, `test` | N/A |\n| Convex | `npx convex` | `dev`, `deploy`, `env set`, `env get` | `npx convex login` |\n\n## Environment Variable Automation\n\n**Env files:** Use Write/Edit tools. Never ask human to create .env manually.\n\n**Dashboard env vars via CLI:**\n\n| Platform | CLI Command | Example |\n|----------|-------------|---------|\n| Convex | `npx convex env set` | `npx convex env set OPENAI_API_KEY sk-...` |\n| Vercel | `vercel env add` | `vercel env add STRIPE_KEY production` |\n| Railway | `railway variables set` | `railway variables set API_KEY=value` |\n| Fly | `fly secrets set` | `fly secrets set DATABASE_URL=...` |\n| Supabase | `supabase secrets set` | `supabase secrets set MY_SECRET=value` |\n\n**Secret collection pattern:**\n```xml\n<!-- WRONG: Asking user to add env vars in dashboard -->\n<task type=\"checkpoint:human-action\">\n  <action>Add OPENAI_API_KEY to Convex dashboard</action>\n  <instructions>Go to dashboard.convex.dev → Settings → Environment Variables → Add</instructions>\n</task>\n\n<!-- RIGHT: Claude asks for value, then adds via CLI -->\n<task type=\"checkpoint:human-action\">\n  <action>Provide your OpenAI API key</action>\n  <instructions>\n    I need your OpenAI API key for Convex backend.\n    Get it from: https://platform.openai.com/api-keys\n    Paste the key (starts with sk-)\n  </instructions>\n  <verification>I'll add it via `npx convex env set` and verify</verification>\n  <resume-signal>Paste your API key</resume-signal>\n</task>\n\n<task type=\"auto\">\n  <name>Configure OpenAI key in Convex</name>\n  <action>Run `npx convex env set OPENAI_API_KEY {user-provided-key}`</action>\n  <verify>`npx convex env get OPENAI_API_KEY` returns the key (masked)</verify>\n</task>\n```\n\n## Dev Server Automation\n\n| Framework | Start Command | Ready Signal | Default URL |\n|-----------|---------------|--------------|-------------|\n| Next.js | `npm run dev` | \"Ready in\" or \"started server\" | http://localhost:3000 |\n| Vite | `npm run dev` | \"ready in\" | http://localhost:5173 |\n| Convex | `npx convex dev` | \"Convex functions ready\" | N/A (backend only) |\n| Express | `npm start` | \"listening on port\" | http://localhost:3000 |\n| Django | `python manage.py runserver` | \"Starting development server\" | http://localhost:8000 |\n\n**Server lifecycle:**\n```bash\n# Run in background, capture PID\nnpm run dev &\nDEV_SERVER_PID=$!\n\n# Wait for ready (max 30s)\ntimeout 30 bash -c 'until curl -s localhost:3000 > /dev/null 2>&1; do sleep 1; done'\n```\n\n**Port conflicts:** Kill stale process (`lsof -ti:3000 | xargs kill`) or use alternate port (`--port 3001`).\n\n**Server stays running** through checkpoints. Only kill when plan complete, switching to production, or port needed for different service.\n\n## CLI Installation Handling\n\n| CLI | Auto-install? | Command |\n|-----|---------------|---------|\n| npm/pnpm/yarn | No - ask user | User chooses package manager |\n| vercel | Yes | `npm i -g vercel` |\n| gh (GitHub) | Yes | `brew install gh` (macOS) or `apt install gh` (Linux) |\n| stripe | Yes | `npm i -g stripe` |\n| supabase | Yes | `npm i -g supabase` |\n| convex | No - use npx | `npx convex` (no install needed) |\n| fly | Yes | `brew install flyctl` or curl installer |\n| railway | Yes | `npm i -g @railway/cli` |\n\n**Protocol:** Try command → \"command not found\" → auto-installable? → yes: install silently, retry → no: checkpoint asking user to install.\n\n## Pre-Checkpoint Automation Failures\n\n| Failure | Response |\n|---------|----------|\n| Server won't start | Check error, fix issue, retry (don't proceed to checkpoint) |\n| Port in use | Kill stale process or use alternate port |\n| Missing dependency | Run `npm install`, retry |\n| Build error | Fix the error first (bug, not checkpoint issue) |\n| Auth error | Create auth gate checkpoint |\n| Network timeout | Retry with backoff, then checkpoint if persistent |\n\n**Never present a checkpoint with broken verification environment.** If `curl localhost:3000` fails, don't ask user to \"visit localhost:3000\".\n\n```xml\n<!-- WRONG: Checkpoint with broken environment -->\n<task type=\"checkpoint:human-verify\">\n  <what-built>Dashboard (server failed to start)</what-built>\n  <how-to-verify>Visit http://localhost:3000...</how-to-verify>\n</task>\n\n<!-- RIGHT: Fix first, then checkpoint -->\n<task type=\"auto\">\n  <name>Fix server startup issue</name>\n  <action>Investigate error, fix root cause, restart server</action>\n  <verify>curl http://localhost:3000 returns 200</verify>\n</task>\n\n<task type=\"checkpoint:human-verify\">\n  <what-built>Dashboard - server running at http://localhost:3000</what-built>\n  <how-to-verify>Visit http://localhost:3000/dashboard...</how-to-verify>\n</task>\n```\n\n## Automatable Quick Reference\n\n| Action | Automatable? | Claude does it? |\n|--------|--------------|-----------------|\n| Deploy to Vercel | Yes (`vercel`) | YES |\n| Create Stripe webhook | Yes (API) | YES |\n| Write .env file | Yes (Write tool) | YES |\n| Create Upstash DB | Yes (`upstash`) | YES |\n| Run tests | Yes (`npm test`) | YES |\n| Start dev server | Yes (`npm run dev`) | YES |\n| Add env vars to Convex | Yes (`npx convex env set`) | YES |\n| Add env vars to Vercel | Yes (`vercel env add`) | YES |\n| Seed database | Yes (CLI/API) | YES |\n| Click email verification link | No | NO |\n| Enter credit card with 3DS | No | NO |\n| Complete OAuth in browser | No | NO |\n| Visually verify UI looks correct | No | NO |\n| Test interactive user flows | No | NO |\n\n</automation_reference>\n\n<writing_guidelines>\n\n**DO:**\n- Automate everything with CLI/API before checkpoint\n- Be specific: \"Visit https://myapp.vercel.app\" not \"check deployment\"\n- Number verification steps\n- State expected outcomes: \"You should see X\"\n- Provide context: why this checkpoint exists\n\n**DON'T:**\n- Ask human to do work Claude can automate ❌\n- Assume knowledge: \"Configure the usual settings\" ❌\n- Skip steps: \"Set up database\" (too vague) ❌\n- Mix multiple verifications in one checkpoint ❌\n\n**Placement:**\n- **After automation completes** - not before Claude does the work\n- **After UI buildout** - before declaring phase complete\n- **Before dependent work** - decisions before implementation\n- **At integration points** - after configuring external services\n\n**Bad placement:** Before automation ❌ | Too frequent ❌ | Too late (dependent tasks already needed the result) ❌\n</writing_guidelines>\n\n<examples>\n\n### Example 1: Database Setup (No Checkpoint Needed)\n\n```xml\n<task type=\"auto\">\n  <name>Create Upstash Redis database</name>\n  <files>.env</files>\n  <action>\n    1. Run `upstash redis create myapp-cache --region us-east-1`\n    2. Capture connection URL from output\n    3. Write to .env: UPSTASH_REDIS_URL={url}\n    4. Verify connection with test command\n  </action>\n  <verify>\n    - upstash redis list shows database\n    - .env contains UPSTASH_REDIS_URL\n    - Test connection succeeds\n  </verify>\n  <done>Redis database created and configured</done>\n</task>\n\n<!-- NO CHECKPOINT NEEDED - Claude automated everything and verified programmatically -->\n```\n\n### Example 2: Full Auth Flow (Single checkpoint at end)\n\n```xml\n<task type=\"auto\">\n  <name>Create user schema</name>\n  <files>src/db/schema.ts</files>\n  <action>Define User, Session, Account tables with Drizzle ORM</action>\n  <verify>npm run db:generate succeeds</verify>\n</task>\n\n<task type=\"auto\">\n  <name>Create auth API routes</name>\n  <files>src/app/api/auth/[...nextauth]/route.ts</files>\n  <action>Set up NextAuth with GitHub provider, JWT strategy</action>\n  <verify>TypeScript compiles, no errors</verify>\n</task>\n\n<task type=\"auto\">\n  <name>Create login UI</name>\n  <files>src/app/login/page.tsx, src/components/LoginButton.tsx</files>\n  <action>Create login page with GitHub OAuth button</action>\n  <verify>npm run build succeeds</verify>\n</task>\n\n<task type=\"auto\">\n  <name>Start dev server for auth testing</name>\n  <action>Run `npm run dev` in background, wait for ready signal</action>\n  <verify>curl http://localhost:3000 returns 200</verify>\n  <done>Dev server running at http://localhost:3000</done>\n</task>\n\n<!-- ONE checkpoint at end verifies the complete flow -->\n<task type=\"checkpoint:human-verify\" gate=\"blocking\">\n  <what-built>Complete authentication flow - dev server running at http://localhost:3000</what-built>\n  <how-to-verify>\n    1. Visit: http://localhost:3000/login\n    2. Click \"Sign in with GitHub\"\n    3. Complete GitHub OAuth flow\n    4. Verify: Redirected to /dashboard, user name displayed\n    5. Refresh page: Session persists\n    6. Click logout: Session cleared\n  </how-to-verify>\n  <resume-signal>Type \"approved\" or describe issues</resume-signal>\n</task>\n```\n</examples>\n\n<anti_patterns>\n\n### ❌ BAD: Asking user to start dev server\n\n```xml\n<task type=\"checkpoint:human-verify\" gate=\"blocking\">\n  <what-built>Dashboard component</what-built>\n  <how-to-verify>\n    1. Run: npm run dev\n    2. Visit: http://localhost:3000/dashboard\n    3. Check layout is correct\n  </how-to-verify>\n</task>\n```\n\n**Why bad:** Claude can run `npm run dev`. User should only visit URLs, not execute commands.\n\n### ✅ GOOD: Claude starts server, user visits\n\n```xml\n<task type=\"auto\">\n  <name>Start dev server</name>\n  <action>Run `npm run dev` in background</action>\n  <verify>curl localhost:3000 returns 200</verify>\n</task>\n\n<task type=\"checkpoint:human-verify\" gate=\"blocking\">\n  <what-built>Dashboard at http://localhost:3000/dashboard (server running)</what-built>\n  <how-to-verify>\n    Visit http://localhost:3000/dashboard and verify:\n    1. Layout matches design\n    2. No console errors\n  </how-to-verify>\n</task>\n```\n\n### ❌ BAD: Asking human to deploy / ✅ GOOD: Claude automates\n\n```xml\n<!-- BAD: Asking user to deploy via dashboard -->\n<task type=\"checkpoint:human-action\" gate=\"blocking\">\n  <action>Deploy to Vercel</action>\n  <instructions>Visit vercel.com/new → Import repo → Click Deploy → Copy URL</instructions>\n</task>\n\n<!-- GOOD: Claude deploys, user verifies -->\n<task type=\"auto\">\n  <name>Deploy to Vercel</name>\n  <action>Run `vercel --yes`. Capture URL.</action>\n  <verify>vercel ls shows deployment, curl returns 200</verify>\n</task>\n\n<task type=\"checkpoint:human-verify\">\n  <what-built>Deployed to {url}</what-built>\n  <how-to-verify>Visit {url}, check homepage loads</how-to-verify>\n  <resume-signal>Type \"approved\"</resume-signal>\n</task>\n```\n\n### ❌ BAD: Too many checkpoints / ✅ GOOD: Single checkpoint\n\n```xml\n<!-- BAD: Checkpoint after every task -->\n<task type=\"auto\">Create schema</task>\n<task type=\"checkpoint:human-verify\">Check schema</task>\n<task type=\"auto\">Create API route</task>\n<task type=\"checkpoint:human-verify\">Check API</task>\n<task type=\"auto\">Create UI form</task>\n<task type=\"checkpoint:human-verify\">Check form</task>\n\n<!-- GOOD: One checkpoint at end -->\n<task type=\"auto\">Create schema</task>\n<task type=\"auto\">Create API route</task>\n<task type=\"auto\">Create UI form</task>\n\n<task type=\"checkpoint:human-verify\">\n  <what-built>Complete auth flow (schema + API + UI)</what-built>\n  <how-to-verify>Test full flow: register, login, access protected page</how-to-verify>\n  <resume-signal>Type \"approved\"</resume-signal>\n</task>\n```\n\n### ❌ BAD: Vague verification / ✅ GOOD: Specific steps\n\n```xml\n<!-- BAD -->\n<task type=\"checkpoint:human-verify\">\n  <what-built>Dashboard</what-built>\n  <how-to-verify>Check it works</how-to-verify>\n</task>\n\n<!-- GOOD -->\n<task type=\"checkpoint:human-verify\">\n  <what-built>Responsive dashboard - server running at http://localhost:3000</what-built>\n  <how-to-verify>\n    Visit http://localhost:3000/dashboard and verify:\n    1. Desktop (>1024px): Sidebar visible, content area fills remaining space\n    2. Tablet (768px): Sidebar collapses to icons\n    3. Mobile (375px): Sidebar hidden, hamburger menu in header\n    4. No horizontal scroll at any size\n  </how-to-verify>\n  <resume-signal>Type \"approved\" or describe layout issues</resume-signal>\n</task>\n```\n\n### ❌ BAD: Asking user to run CLI commands\n\n```xml\n<task type=\"checkpoint:human-action\">\n  <action>Run database migrations</action>\n  <instructions>Run: npx prisma migrate deploy && npx prisma db seed</instructions>\n</task>\n```\n\n**Why bad:** Claude can run these commands. User should never execute CLI commands.\n\n### ❌ BAD: Asking user to copy values between services\n\n```xml\n<task type=\"checkpoint:human-action\">\n  <action>Configure webhook URL in Stripe</action>\n  <instructions>Copy deployment URL → Stripe Dashboard → Webhooks → Add endpoint → Copy secret → Add to .env</instructions>\n</task>\n```\n\n**Why bad:** Stripe has an API. Claude should create the webhook via API and write to .env directly.\n\n</anti_patterns>\n\n<summary>\n\nCheckpoints formalize human-in-the-loop points for verification and decisions, not manual work.\n\n**The golden rule:** If Claude CAN automate it, Claude MUST automate it.\n\n**Checkpoint priority:**\n1. **checkpoint:human-verify** (90%) - Claude automated everything, human confirms visual/functional correctness\n2. **checkpoint:decision** (9%) - Human makes architectural/technology choices\n3. **checkpoint:human-action** (1%) - Truly unavoidable manual steps with no API/CLI\n\n**When NOT to use checkpoints:**\n- Things Claude can verify programmatically (tests, builds)\n- File operations (Claude can read files)\n- Code correctness (tests and static analysis)\n- Anything automatable via CLI/API\n</summary>\n\n       <overview>\nTDD is about design quality, not coverage metrics. The red-green-refactor cycle forces you to think about behavior before implementation, producing cleaner interfaces and more testable code.\n\n**Principle:** If you can describe the behavior as `expect(fn(input)).toBe(output)` before writing `fn`, TDD improves the result.\n\n**Key insight:** TDD work is fundamentally heavier than standard tasks—it requires 2-3 execution cycles (RED → GREEN → REFACTOR), each with file reads, test runs, and potential debugging. TDD features get dedicated plans to ensure full context is available throughout the cycle.\n</overview>\n\n<when_to_use_tdd>\n## When TDD Improves Quality\n\n**TDD candidates (create a TDD plan):**\n- Business logic with defined inputs/outputs\n- API endpoints with request/response contracts\n- Data transformations, parsing, formatting\n- Validation rules and constraints\n- Algorithms with testable behavior\n- State machines and workflows\n- Utility functions with clear specifications\n\n**Skip TDD (use standard plan with `type=\"auto\"` tasks):**\n- UI layout, styling, visual components\n- Configuration changes\n- Glue code connecting existing components\n- One-off scripts and migrations\n- Simple CRUD with no business logic\n- Exploratory prototyping\n\n**Heuristic:** Can you write `expect(fn(input)).toBe(output)` before writing `fn`?\n→ Yes: Create a TDD plan\n→ No: Use standard plan, add tests after if needed\n</when_to_use_tdd>\n\n<tdd_plan_structure>\n## TDD Plan Structure\n\nEach TDD plan implements **one feature** through the full RED-GREEN-REFACTOR cycle.\n\n```markdown\n---\nphase: XX-name\nplan: NN\ntype: tdd\n---\n\n<objective>\n[What feature and why]\nPurpose: [Design benefit of TDD for this feature]\nOutput: [Working, tested feature]\n</objective>\n\n<context>\n@.planning/PROJECT.md\n@.planning/ROADMAP.md\n@relevant/source/files.ts\n</context>\n\n<feature>\n  <name>[Feature name]</name>\n  <files>[source file, test file]</files>\n  <behavior>\n    [Expected behavior in testable terms]\n    Cases: input → expected output\n  </behavior>\n  <implementation>[How to implement once tests pass]</implementation>\n</feature>\n\n<verification>\n[Test command that proves feature works]\n</verification>\n\n<success_criteria>\n- Failing test written and committed\n- Implementation passes test\n- Refactor complete (if needed)\n- All 2-3 commits present\n</success_criteria>\n\n<output>\nAfter completion, create SUMMARY.md with:\n- RED: What test was written, why it failed\n- GREEN: What implementation made it pass\n- REFACTOR: What cleanup was done (if any)\n- Commits: List of commits produced\n</output>\n```\n\n**One feature per TDD plan.** If features are trivial enough to batch, they're trivial enough to skip TDD—use a standard plan and add tests after.\n</tdd_plan_structure>\n\n<execution_flow>\n## Red-Green-Refactor Cycle\n\n**RED - Write failing test:**\n1. Create test file following project conventions\n2. Write test describing expected behavior (from `<behavior>` element)\n3. Run test - it MUST fail\n4. If test passes: feature exists or test is wrong. Investigate.\n5. Commit: `test({phase}-{plan}): add failing test for [feature]`\n\n**GREEN - Implement to pass:**\n1. Write minimal code to make test pass\n2. No cleverness, no optimization - just make it work\n3. Run test - it MUST pass\n4. Commit: `feat({phase}-{plan}): implement [feature]`\n\n**REFACTOR (if needed):**\n1. Clean up implementation if obvious improvements exist\n2. Run tests - MUST still pass\n3. Only commit if changes made: `refactor({phase}-{plan}): clean up [feature]`\n\n**Result:** Each TDD plan produces 2-3 atomic commits.\n</execution_flow>\n\n<test_quality>\n## Good Tests vs Bad Tests\n\n**Test behavior, not implementation:**\n- Good: \"returns formatted date string\"\n- Bad: \"calls formatDate helper with correct params\"\n- Tests should survive refactors\n\n**One concept per test:**\n- Good: Separate tests for valid input, empty input, malformed input\n- Bad: Single test checking all edge cases with multiple assertions\n\n**Descriptive names:**\n- Good: \"should reject empty email\", \"returns null for invalid ID\"\n- Bad: \"test1\", \"handles error\", \"works correctly\"\n\n**No implementation details:**\n- Good: Test public API, observable behavior\n- Bad: Mock internals, test private methods, assert on internal state\n</test_quality>\n\n<framework_setup>\n## Test Framework Setup (If None Exists)\n\nWhen executing a TDD plan but no test framework is configured, set it up as part of the RED phase:\n\n**1. Detect project type:**\n```bash\n# JavaScript/TypeScript\nif [ -f package.json ]; then echo \"node\"; fi\n\n# Python\nif [ -f requirements.txt ] || [ -f pyproject.toml ]; then echo \"python\"; fi\n\n# Go\nif [ -f go.mod ]; then echo \"go\"; fi\n\n# Rust\nif [ -f Cargo.toml ]; then echo \"rust\"; fi\n```\n\n**2. Install minimal framework:**\n| Project | Framework | Install |\n|---------|-----------|---------|\n| Node.js | Jest | `npm install -D jest @types/jest ts-jest` |\n| Node.js (Vite) | Vitest | `npm install -D vitest` |\n| Python | pytest | `pip install pytest` |\n| Go | testing | Built-in |\n| Rust | cargo test | Built-in |\n\n**3. Create config if needed:**\n- Jest: `jest.config.js` with ts-jest preset\n- Vitest: `vitest.config.ts` with test globals\n- pytest: `pytest.ini` or `pyproject.toml` section\n\n**4. Verify setup:**\n```bash\n# Run empty test suite - should pass with 0 tests\nnpm test  # Node\npytest    # Python\ngo test ./...  # Go\ncargo test    # Rust\n```\n\n**5. Create first test file:**\nFollow project conventions for test location:\n- `*.test.ts` / `*.spec.ts` next to source\n- `__tests__/` directory\n- `tests/` directory at root\n\nFramework setup is a one-time cost included in the first TDD plan's RED phase.\n</framework_setup>\n\n<error_handling>\n## Error Handling\n\n**Test doesn't fail in RED phase:**\n- Feature may already exist - investigate\n- Test may be wrong (not testing what you think)\n- Fix before proceeding\n\n**Test doesn't pass in GREEN phase:**\n- Debug implementation\n- Don't skip to refactor\n- Keep iterating until green\n\n**Tests fail in REFACTOR phase:**\n- Undo refactor\n- Commit was premature\n- Refactor in smaller steps\n\n**Unrelated tests break:**\n- Stop and investigate\n- May indicate coupling issue\n- Fix before proceeding\n</error_handling>\n\n<commit_pattern>\n## Commit Pattern for TDD Plans\n\nTDD plans produce 2-3 atomic commits (one per phase):\n\n```\ntest(08-02): add failing test for email validation\n\n- Tests valid email formats accepted\n- Tests invalid formats rejected\n- Tests empty input handling\n\nfeat(08-02): implement email validation\n\n- Regex pattern matches RFC 5322\n- Returns boolean for validity\n- Handles edge cases (empty, null)\n\nrefactor(08-02): extract regex to constant (optional)\n\n- Moved pattern to EMAIL_REGEX constant\n- No behavior changes\n- Tests still pass\n```\n\n**Comparison with standard plans:**\n- Standard plans: 1 commit per task, 2-4 commits per plan\n- TDD plans: 2-3 commits for single feature\n\nBoth follow same format: `{type}({phase}-{plan}): {description}`\n\n**Benefits:**\n- Each commit independently revertable\n- Git bisect works at commit level\n- Clear history showing TDD discipline\n- Consistent with overall commit strategy\n</commit_pattern>\n\n<context_budget>\n## Context Budget\n\nTDD plans target **~40% context usage** (lower than standard plans' ~50%).\n\nWhy lower:\n- RED phase: write test, run test, potentially debug why it didn't fail\n- GREEN phase: implement, run test, potentially iterate on failures\n- REFACTOR phase: modify code, run tests, verify no regressions\n\nEach phase involves reading files, running commands, analyzing output. The back-and-forth is inherently heavier than linear task execution.\n\nSingle feature focus ensures full quality throughout the cycle.\n</context_budget>\n\n       </execution_context>\n\n       <files_to_read>\n       Read these files at execution start using the Read tool:\n       - Plan: {phase_dir}/{plan_file}\n       - State: .planning/STATE.md\n       - Config: .planning/config.json (if exists)\n       </files_to_read>\n\n       <success_criteria>\n       - [ ] All tasks executed\n       - [ ] Each task committed individually\n       - [ ] SUMMARY.md created in plan directory\n       - [ ] STATE.md updated with position and decisions\n       - [ ] ROADMAP.md updated with plan progress (via `gsd_roadmap_update_plan_progress` tool)\n       </success_criteria>\n</task_prompt>\n\nAfter completion: keep the same completion markers expected by the workflow and continue.\n</single_agent_step>"
    },
    {
      "number": "2",
      "content": "<single_agent_step number=\"2\">\nExecute this pass directly in the current conversation (MCP-safe mode: no delegated call).\nOriginal delegated task: Verify Phase {phase_number} goal achievement\n\n<task_prompt>\nVerify phase {phase_number} goal achievement.\n    Phase directory: {phase_dir}\n    Phase goal: {goal from ROADMAP.md}\n    Phase requirement IDs: {phase_req_ids}\n    Check must_haves against actual codebase.\n    Cross-reference requirement IDs from PLAN frontmatter against REQUIREMENTS.md — every ID MUST be accounted for.\n    Create VERIFICATION.md.\n</task_prompt>\n\nAfter completion: keep the same completion markers expected by the workflow and continue.\n</single_agent_step>"
    }
  ],
  "history": [
    {
      "step": 1,
      "total": 2,
      "result": "retry",
      "notes": "Completed 12-01 including three task commits (d60387a, 7e4a9fe, 4758e86) and metadata commit 0caced5. Remaining items: 12-02, 12-03.",
      "timestamp": "2026-02-19T23:45:14.875Z"
    },
    {
      "step": 1,
      "total": 2,
      "result": "retry",
      "notes": "Completed 12-02 with task commits 59c540b, 51f0a71, 26610cc, deviation fix f975890, and metadata commit 96e8e07. Remaining item: 12-03.",
      "timestamp": "2026-02-19T23:52:35.202Z"
    },
    {
      "step": 1,
      "total": 2,
      "result": "done",
      "notes": "Completed 12-03 with commits 65812bf, aa640c2, e0f8ae1 plus startup smoke fixes 9d96192 and 945c7b5, and metadata commit 1a5b50f. All phase plans now have summaries.",
      "timestamp": "2026-02-20T00:06:01.961Z"
    },
    {
      "step": 2,
      "total": 2,
      "result": "done",
      "notes": "",
      "timestamp": "2026-02-20T00:07:43.742Z"
    }
  ]
}